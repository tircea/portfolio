import nltk
import random
import matplotlib.pyplot as plt
from nltk.corpus import movie_reviews
from nltk import FreqDist
from nltk.classify import NaiveBayesClassifier, SklearnClassifier
from nltk.classify.util import accuracy
from sklearn.svm import SVC

# –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –∫–æ—Ä–ø—É—Å—É
nltk.download('movie_reviews')

print("=== –°–µ–Ω—Ç–∏–º–µ–Ω—Ç-–∞–Ω–∞–ª—ñ–∑ —Ñ—ñ–ª—å–º–æ–≤–∏—Ö –≤—ñ–¥–≥—É–∫—ñ–≤ ===")
print("–ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –¥–∞–Ω–∏—Ö...")

# –û—Ç—Ä–∏–º–∞–Ω–Ω—è —Å–ø–∏—Å–∫—É –≤—ñ–¥–≥—É–∫—ñ–≤ —Ç–∞ —ó—Ö –∫–∞—Ç–µ–≥–æ—Ä—ñ–π
reviews = []
for category in movie_reviews.categories():
    for fileid in movie_reviews.fileids(category):
        review_words = list(movie_reviews.words(fileid))
        reviews.append((review_words, category))

random.shuffle(reviews)

# –§–æ—Ä–º—É–≤–∞–Ω–Ω—è —á–∞—Å—Ç–æ—Ç–Ω–æ–≥–æ —Å–ª–æ–≤–Ω–∏–∫–∞ (–±–µ–∑ –ø—É–Ω–∫—Ç—É–∞—Ü—ñ—ó)
all_words = [word.lower() for word in movie_reviews.words() if word.isalpha()]
all_words_freq = FreqDist(all_words)

# –§—É–Ω–∫—Ü—ñ—è: –ø–æ–±—É–¥–æ–≤–∞ –≥—Ä–∞—Ñ—ñ–∫–∞ —á–∞—Å—Ç–æ—Ç–Ω–∏—Ö —Å–ª—ñ–≤
def plot_top_words(freq_dist, n=20):
    top_words = freq_dist.most_common(n)
    words = [w for w, _ in top_words]
    counts = [c for _, c in top_words]

    plt.figure(figsize=(12, 6))
    plt.bar(words, counts)
    plt.title(f"–¢–æ–ø {n} –Ω–∞–π—á–∞—Å—Ç–æ—Ç–Ω—ñ—à–∏—Ö —Å–ª—ñ–≤ —É –∫–æ—Ä–ø—É—Å—ñ movie_reviews")
    plt.xlabel("–°–ª–æ–≤–∞")
    plt.ylabel("–ß–∞—Å—Ç–æ—Ç–∞")
    plt.xticks(rotation=45)
    plt.tight_layout()
    plt.show()

# –û–ø—Ü—ñ—è: –ø–æ–∫–∞–∑–∞—Ç–∏ —Ç–æ–ø-—Å–ª—ñ–≤
show_graph = input("–ë–∞–∂–∞—î—Ç–µ –ø–æ–±–∞—á–∏—Ç–∏ –≥—Ä–∞—Ñ—ñ–∫ –Ω–∞–π—á–∞—Å—Ç–æ—Ç–Ω—ñ—à–∏—Ö —Å–ª—ñ–≤ —É –∫–æ—Ä–ø—É—Å—ñ? (—Ç–∞–∫/–Ω—ñ): ").strip().lower()
if show_graph == '—Ç–∞–∫':
    print("\nüîé –¶–µ–π –≥—Ä–∞—Ñ—ñ–∫ –¥–µ–º–æ–Ω—Å—Ç—Ä—É—î 20 –Ω–∞–π—É–∂–∏–≤–∞–Ω—ñ—à–∏—Ö —Å–ª—ñ–≤ —É –∫–æ—Ä–ø—É—Å—ñ –∞–Ω–≥–ª–æ–º–æ–≤–Ω–∏—Ö —Ñ—ñ–ª—å–º–æ–≤–∏—Ö –≤—ñ–¥–≥—É–∫—ñ–≤ –∑ NLTK (movie_reviews).")
    plot_top_words(all_words_freq)

# –°–ª–æ–≤–∞, —è–∫—ñ –º–æ–∂–Ω–∞ –æ–±—Ä–∞—Ç–∏ –¥–ª—è –∞–Ω–∞–ª—ñ–∑—É
available_words = ["good", "bad", "practical", "fun", "boring"]
print("\n–û–±–µ—Ä—ñ—Ç—å —Å–ª–æ–≤–æ –¥–ª—è –∞–Ω–∞–ª—ñ–∑—É –∑ —Ü—å–æ–≥–æ —Å–ø–∏—Å–∫—É:")
print(", ".join(available_words))
selected_word = input("–í–≤–µ–¥—ñ—Ç—å —Å–ª–æ–≤–æ: ").strip().lower()

# –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞, —á–∏ —Å–ª–æ–≤–æ –∑ –¥–æ—Å—Ç—É–ø–Ω–æ–≥–æ —Å–ø–∏—Å–∫—É
if selected_word not in available_words:
    print("‚ùå –°–ª–æ–≤–æ –Ω–µ –∑ –¥–æ—Å—Ç—É–ø–Ω–æ–≥–æ —Å–ø–∏—Å–∫—É. –ó–∞–≤–µ—Ä—à–µ–Ω–Ω—è –ø—Ä–æ–≥—Ä–∞–º–∏.")
    exit()

# –ß–∞—Å—Ç–æ—Ç–∞ —Å–ª–æ–≤–∞ –≤ —É—Å—å–æ–º—É –∫–æ—Ä–ø—É—Å—ñ, –ø–æ–∑–∏—Ç–∏–≤–Ω–∏—Ö —ñ –Ω–µ–≥–∞—Ç–∏–≤–Ω–∏—Ö –≤—ñ–¥–≥—É–∫–∞—Ö
total_count = all_words_freq[selected_word]
positive_reviews = [w.lower() for w in movie_reviews.words(categories='pos')]
negative_reviews = [w.lower() for w in movie_reviews.words(categories='neg')]
pos_count = FreqDist(positive_reviews)[selected_word]
neg_count = FreqDist(negative_reviews)[selected_word]

# –í–∏–≤—ñ–¥ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
print(f"\n–°–ª–æ–≤–æ ¬´{selected_word}¬ª –∑–∞–≥–∞–ª–æ–º –∑—É—Å—Ç—Ä—ñ—á–∞—î—Ç—å—Å—è: {total_count} —Ä–∞–∑(—ñ–≤)")
print(f"–£ –ø–æ–∑–∏—Ç–∏–≤–Ω–∏—Ö –≤—ñ–¥–≥—É–∫–∞—Ö: {pos_count}")
print(f"–£ –Ω–µ–≥–∞—Ç–∏–≤–Ω–∏—Ö –≤—ñ–¥–≥—É–∫–∞—Ö: {neg_count}")

# –ü–æ–±—É–¥–æ–≤–∞ –æ–∑–Ω–∞–∫ –¥–ª—è –∫–ª–∞—Å–∏—Ñ—ñ–∫–∞—Ç–æ—Ä—ñ–≤
word_features = list(all_words_freq)[:3600]
def find_features(document_words):
    words = set(w.lower() for w in document_words if w.isalpha())
    return {w: (w in words) for w in word_features}

featuresets = [(find_features(words), category) for (words, category) in reviews]
random.shuffle(featuresets)
train_set, test_set = featuresets[:1800], featuresets[1800:]

# –ù–∞–≤—á–∞–Ω–Ω—è Naive Bayes
classifier = NaiveBayesClassifier.train(train_set)
nb_accuracy = accuracy(classifier, test_set) * 100
print(f"\n –¢–æ—á–Ω—ñ—Å—Ç—å –∫–ª–∞—Å–∏—Ñ—ñ–∫–∞—Ç–æ—Ä–∞ Naive Bayes: {nb_accuracy:.2f}%")

# –ù–∞–≤—á–∞–Ω–Ω—è SVC
svc_classifier = SklearnClassifier(SVC(kernel='linear'))
svc_classifier.train(train_set)
svc_accuracy = accuracy(svc_classifier, test_set) * 100
print(f" –¢–æ—á–Ω—ñ—Å—Ç—å –∫–ª–∞—Å–∏—Ñ—ñ–∫–∞—Ç–æ—Ä–∞ SVC: {svc_accuracy:.2f}%")

# –ü–æ–±—É–¥–æ–≤–∞ –≥—Ä–∞—Ñ—ñ–∫–∞ —Ç–æ—á–Ω–æ—Å—Ç—ñ
def plot_accuracy(nb_acc, svc_acc):
    classifiers = ['Naive Bayes', 'SVC']
    accuracies = [nb_acc, svc_acc]

    plt.figure(figsize=(6, 4))
    plt.bar(classifiers, accuracies, color=['skyblue', 'lightgreen'])
    plt.ylabel('–¢–æ—á–Ω—ñ—Å—Ç—å (%)')
    plt.title('–ü–æ—Ä—ñ–≤–Ω—è–Ω–Ω—è –∫–ª–∞—Å–∏—Ñ—ñ–∫–∞—Ç–æ—Ä—ñ–≤')
    plt.ylim(0, 100)
    for i, acc in enumerate(accuracies):
        plt.text(i, acc + 1, f"{acc:.2f}%", ha='center')
    plt.tight_layout()
    plt.show()

# –û–ø—Ü—ñ—è: –ø–æ–∫–∞–∑–∞—Ç–∏ –≥—Ä–∞—Ñ—ñ–∫ –ø–æ—Ä—ñ–≤–Ω—è–Ω–Ω—è –∫–ª–∞—Å–∏—Ñ—ñ–∫–∞—Ç–æ—Ä—ñ–≤
show_acc = input("\n–ë–∞–∂–∞—î—Ç–µ –ø–æ–±–∞—á–∏—Ç–∏ –≥—Ä–∞—Ñ—ñ–∫ —Ç–æ—á–Ω–æ—Å—Ç—ñ –∫–ª–∞—Å–∏—Ñ—ñ–∫–∞—Ç–æ—Ä—ñ–≤? (—Ç–∞–∫/–Ω—ñ): ").strip().lower()
if show_acc == '—Ç–∞–∫':
    plot_accuracy(nb_accuracy, svc_accuracy)

# –ó–±–µ—Ä–µ–∂–µ–Ω–Ω—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ñ–≤
with open("results.txt", "w", encoding="utf-8") as f:
    f.write("–†–µ–∑—É–ª—å—Ç–∞—Ç–∏ —Å–µ–Ω—Ç–∏–º–µ–Ω—Ç-–∞–Ω–∞–ª—ñ–∑—É\n")
    f.write(f"–°–ª–æ–≤–æ: {selected_word}\n")
    f.write(f"–ó–∞–≥–∞–ª—å–Ω–∞ —á–∞—Å—Ç–æ—Ç–∞: {total_count}\n")
    f.write(f"–£ –ø–æ–∑–∏—Ç–∏–≤–Ω–∏—Ö: {pos_count}\n")
    f.write(f"–£ –Ω–µ–≥–∞—Ç–∏–≤–Ω–∏—Ö: {neg_count}\n")
    f.write(f"–¢–æ—á–Ω—ñ—Å—Ç—å Naive Bayes: {nb_accuracy:.2f}%\n")
    f.write(f"–¢–æ—á–Ω—ñ—Å—Ç—å SVC: {svc_accuracy:.2f}%\n")

print("\n –ê–Ω–∞–ª—ñ–∑ –∑–∞–≤–µ—Ä—à–µ–Ω–æ. –†–µ–∑—É–ª—å—Ç–∞—Ç–∏ –∑–±–µ—Ä–µ–∂–µ–Ω–æ —É —Ñ–∞–π–ª 'results.txt'.")